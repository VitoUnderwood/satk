# sentiment analysis tool kit

## 介绍

情感分析工具集合
web项目：
爬虫项目：

## 环境

python 3.7
transoformers
pytorch
gensim

## 使用说明

```bash
pip install satk
import satk
```

| 功能         | 模型            | 实现 |
| ------------ | --------------- | ---- |
| 新闻主题分类 | news_classifier | bert |
| 评论主题分布 | lda             | LDA  |

## 训练数据

数据源准备
新浪数据
评论合并 100左右长度，

| 主题     | 新闻数量 | 评论数量 |
| -------- | -------- | -------- |
| 体育     |
| 财经     |
| 时尚     |
| 科技数码 |
| 娱乐八卦 |
| 汽车     |
| 房产     |
| 教育     |
| 游戏     |
| 政治军事 |
通过实验可以发现，还是覆盖范围太广，主题之间的交叉重叠太明显
主题范围太广，需要大量的数据来支撑
先选择一些热点时间比较狭窄范围

### 数据预处理

- 繁体转简体  # 繁体字处理，不然会有冗余，没有转换之前Dictionary(305734 unique tokens: ['五角星', '件', '伦伦', '余', '各種']...)
        # 处理后的结果 Dictionary(287951 unique tokens: ['五角星', '件', '伦伦', '余', '周小伦']...)

### 基于频率

基于频率的主题分布预测通过对特定领域评论中大量出现的名词，名词短语的频率进行统计操作
通过实验来确定阈值，保留大于阈值的名词。

因为人们在评论不同的主题时，常用比较固定的类似的词语。因此频繁出现的名词通常就是重要的具有代表性的词语
前提要去除掉高频出现的stop word， 通常和主题无关，但是对统计频率有很大的影响
对于和主题相关性较低的词语在不同的主题下频率相对较低

基本假设：语料库中有相当数量的评论，而且评论按照主题划分，每个主题下的评论不能大量混合其他无关评论。
每一个主题都具有独特的词概率分布，具有一定的区分主题的能力。
LDA刻画文档生成的概率化过程，LDA基于贝叶斯网络

LDA 认为每篇文章是由多个主题 mix 混合而成的，而每个主题可以由多个词
的概率表征

- 存在问题：不适用短文本，但品论是典型的短文本，任务爬取的是新闻评论，单独新闻的所有评论聚合成为一个长文本，可以进行对比实验
- 评论数据清洗，制定规则

- 输入：一组文档的组成的词料库
- 输出：文档的主题分布，主题的词语分布

候选名词筛选

设想：先进行实体抽取，POS，BIO 然后再进行TF-IDF， LDA主题聚类算法
后期可能根据需要构建自己的stop word来提升效果
PMI互信息，计算共现

## 评价指标

1. 困惑度
2. 连贯度

## 实验过程

1. 构建爬虫，确定主题（区分度较大和近似的），不进行任何预处理，直接使用LDA处理
明显可以看出，有的主题分类很明显，有的很模糊，比如舟曲泥石流，星座的，很多都是针对某一个比较具体的事件产生的评论，聚类效果显著，因此接下来打算爬取针对性的数据来构建预料
2 - 30 topic 连贯度虽然在上升，但总体比较低，0.35，还有上升的空间，测试20-50-3的结果，依旧最高在30个topic附近徘徊，0.36左右

2. 分析结果发现除了常见的停用词之外，还有很多无法很好表达主题词语在其中，还占据了相当大比例，所以构建新的停用词 

```python
[(2,
  '0.014*"说" + 0.008*"工作" + 0.007*"孩子" + 0.006*"想" + 0.005*"母亲" + 0.005*"做" + 0.004*"公司" + 0.004*"儿子" + 0.003*"生活" + 0.003*"明天"'),
 (6,
  '0.019*"哈哈哈" + 0.017*"说" + 0.007*"做" + 0.005*"真的" + 0.005*"想" + 0.004*"写" + 0.004*"结婚" + 0.004*"哈哈哈哈" + 0.004*"回复" + 0.004*"话"'),
 (25,
  '0.005*"回复" + 0.005*"中" + 0.005*"找" + 0.004*"想" + 0.004*"说" + 0.004*"真" + 0.003*"名字" + 0.003*"不错" + 0.003*"走" + 0.003*"照片"'),
 (16,
  '0.019*"说" + 0.006*"想" + 0.006*"真的" + 0.004*"宝宝" + 0.004*"里" + 0.004*"晚安" + 0.004*"上班" + 0.004*"太" + 0.004*"不想" + 0.003*"真"'),
 (4,
  '0.043*"吃" + 0.014*"喜欢" + 0.013*"想" + 0.008*"说" + 0.005*"做" + 0.005*"爱" + 0.005*"饿" + 0.005*"回复" + 0.004*"吃饭" + 0.004*"真"'),
 (23,
  '0.011*"唔" + 0.006*"遇难" + 0.006*"老人" + 0.006*"广州" + 0.004*"满面" + 0.004*"太" + 0.004*"表情" + 0.004*"说" + 0.003*"北京" + 0.003*"可爱"'),
 (27,
  '0.008*"中" + 0.006*"做" + 0.006*"里" + 0.005*"说" + 0.005*"真的" + 0.005*"回复" + 0.005*"想" + 0.003*"买" + 0.003*"生活" + 0.003*"中国"'),
 (17,
  '0.013*"男" + 0.013*"女" + 0.007*"回复" + 0.006*"天蝎" + 0.006*"射手" + 0.006*"金牛" + 0.006*"水瓶" + 0.006*"狮子" + 0.006*"巨蟹" + 0.006*"双鱼"'),
 (14,
  '0.024*"啊啊啊" + 0.011*"加油" + 0.010*"微博" + 0.010*"希望" + 0.008*"想" + 0.006*"谢谢" + 0.006*"请" + 0.005*"朋友" + 0.005*"回复" + 0.005*"新浪"'),
 (10,
  '0.014*"说" + 0.008*"回复" + 0.006*"想" + 0.005*"喜欢" + 0.005*"朋友" + 0.004*"猫" + 0.004*"走" + 0.004*"太" + 0.004*"请" + 0.004*"一只"'),
 (3,
  '0.014*"岁" + 0.008*"钱" + 0.006*"回复" + 0.006*"买" + 0.005*"太" + 0.005*"票" + 0.005*"吃" + 0.005*"说" + 0.005*"生日" + 0.004*"感谢"'),
 (0,
  '0.004*"想" + 0.004*"擦" + 0.004*"中" + 0.003*"分钟" + 0.003*"喜欢" + 0.003*"生活" + 0.003*"考试" + 0.003*"演唱会" + 0.003*"小时" + 0.003*"回复"'),
 (12,
  '0.009*"死" + 0.009*"喜欢" + 0.007*"做" + 0.006*"人生" + 0.006*"爱" + 0.005*"时间" + 0.005*"开心" + 0.005*"笑" + 0.005*"日本" + 0.004*"说"'),
 (7,
  '0.027*"第名" + 0.013*"星座" + 0.012*"天蝎座" + 0.011*"射手座" + 0.011*"狮子座" + 0.011*"巨蟹座" + 0.011*"金牛座" + 0.011*"水瓶座" + 0.011*"处女座" + 0.011*"双鱼座"'),
 (26,
  '0.026*"中国" + 0.011*"说" + 0.009*"日本" + 0.004*"美国" + 0.004*"太" + 0.004*"总统" + 0.003*"做" + 0.003*"菲律宾" + 0.003*"事件" + 0.003*"回复"'),
 (5,
  '0.013*"幸福" + 0.010*"舟曲" + 0.005*"回复" + 0.005*"说" + 0.004*"泥石流" + 0.004*"想" + 0.004*"死" + 0.004*"讨厌" + 0.004*"嘤" + 0.004*"岁"'),
 (9,
  '0.013*"女人" + 0.013*"做" + 0.012*"妈妈" + 0.010*"太" + 0.010*"可爱" + 0.009*"孩子" + 0.007*"菲律宾" + 0.007*"男人" + 0.006*"城管" + 0.006*"肉"'),
 (15,
  '0.019*"爱" + 0.011*"说" + 0.006*"回复" + 0.004*"想" + 0.003*"一种" + 0.003*"生命" + 0.003*"永远" + 0.003*"朋友" + 0.003*"快乐" + 0.003*"做"'),
 (11,
  '0.007*"想" + 0.005*"说" + 0.005*"朋友" + 0.005*"中" + 0.004*"回复" + 0.004*"谢谢" + 0.004*"嗷嗷" + 0.004*"买" + 0.004*"生日快乐" + 0.003*"太"'),
 (22,
  '0.014*"男人" + 0.011*"女人" + 0.009*"围脖" + 0.004*"中国" + 0.004*"选择" + 0.004*"回复" + 0.004*"钱" + 0.003*"说" + 0.003*"刚刚" + 0.003*"支持"')]
```

2. 长短文本的对比实验 
优化提升 自动确定k值 Gap Statistic

## 模型下载

## 未完待续

- [ ] 详细使用文档
- [ ] 模型共享
- [ ] lda word2vec
