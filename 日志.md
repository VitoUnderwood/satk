# ABSA平台开发日志

## 2020.09.21

- 讨论平台的整体设计思路，确定了基本功能和创新点
- 为了防止功亏一篑，搞定git，youtube git开发流程, 基本熟悉了分支流程，分支合并，但是多人协作还是不太清楚
- 在dev上创建新分支，开发新功能，然后merge到dev上，删除掉该分支，测试没问题后，在发布到master上
- 拿到学长论文
- 梳理项目

## 2020.09.22

进行了git项目实战， feature->dev->master, 整理项目内容
阅读学长论文，解读模型架构
收集semeval数据

## 2020.09.23

进行xml数据预处理
研究学长代码，整理简化框架，适应新版本
解决包与包之间路径混乱问题
添加进度条，增加训练过程可读性，from tqdm import tqdm

突破口：NER

常用NER标注体系：共有4种标注体系，最常用的还是BIO。BIO标注：将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

<img src="./img/NER标注体系.jpg" width = "50%" />

```python
# model tensor loss_fun
设置bert模型缓存位置代码 from_pretrain('name', cache_dir='path_to_svae)
指定设备 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.cuda()
tensor = tensor.cuda()
model = model.to(device)
tensor = tensor.to(device)
loss = nn.CrossEntropyLoss().to(device)
```

### 存在问题

- 无法自动切换cup，gpu训练
- 代码冗余，太多实验代码，不够纯净
- 缺少良好的用户命令行接口 docopt argparse

## 2020.09.24

tokenize:仅用来分词，不进行2idx的操作， bert的分词自动识别标点，不用考虑是否有空格
首先进行tagset的标记，利用tag的idx标记tokens，然后padding
{'POS': 0, 'NEU': 1, 'NEG': 2}

- feature [CLS] + bert token id + [SEP] (idx, input_ids, input_mask, segment_ids)
- TS target start [0,0,0,0,....1,....0]
- TE target end [0,0,0,0,....1,....0]
- TP target polarity [-1, -1, -1,... (0,1,2), ....-1, -1]
- TN target number 一句话里可能由多个target
- OS opinion start
- OE opinion end
- OP opinion polarity
- ON opinion number 一个target可能有多个opinion
- TOS 128x128 整合TS到一个矩阵中
- TOE 128x128
- TON 128

```python
 def forward(self, input_ids, attention_mask, token_type_ids,
                start_positions_target, end_positions_target, polarity_target, n_target,
                start_positions_opinion, end_positions_opinion, polarity_opinion, n_opinion,
                start_positions_target_opinion, end_positions_target_opinion, n_target_opinion):
```

数据的数值化处理已经解读完毕，开始模型解读

## 2020.09.25

和学长交流后，方向调整了一下
阅读论文
和导师讨论，调整新方向，任务如下

## 2020.09.26

可视化lda的结果，分别训练3-30个topic
学习Django建站
django-admin startproject data_science
python manage.py migrate
python manage.py createsuperuser
startapp myapp

request -> url -> views -> models -> templates

添加页面

- app/views.py 编写页面响应函数
- app/urls.py 添加应用下的路径
- project/urls.py include app urls

## 2020.09.27

lda结果的可解释性

网站数据库交互
app/models 定义数据结构
project/settings 注册app app.apps.AppConfig
python manage.py makemigrations
python manage.py migrate 生效

后台管理数据库，在admin.py注册对应的表

构建模版
app/templates/app/ *.html
render(request, html_path, data_dict)
path name call for template tag

404页面丢失
get_object_or_404

使用name是要添加命名空间，养成良好的习惯
搭建了基本网站 django + bootstrap

## 2020.09.28

添加lda入库
连接网站
实现post请求成功，必须在form下面添加csrf_token

完成前后端的交互，实现基本功能

初步实现头条爬虫， 使用selenium来模拟浏览器，解决重定向问题
新目标，速度太慢，尝试多线程提速

## 总目标

1. 解读学习学长代码结构，摸索代码规范

2. 熟悉git

3. 寻找论文 bo pang liu bing，实现baseline

4. 实现semeval的经典代码

5. 完善包的结构

6. 参考网站<http://xiaosi.trs.cn/demo/rs/demo>

7. 模型的发布和线上加载，借助transoformers平台

8. spanBert

9. 常用关键词抽取方法

10. 搞清楚bert详细的输入输出结构

- 假如要做一个demo，其特色：

    输出：和semeval的输出格式相同

    输入：类型1： 训练集（semeval标注语料）

    ​      类型2：无监督或者半监督的语料。

    0、故事：包括semeval提供的例子。但拿其做个靶子（有标注的、特定领域的），我们能否做一个无标注的、或者少量标注的、开放域的（或者迁移性较好的）

    1、开源系统；github，pip install；

    2、平台。包括该方向的经典算法：如 1, 受 liu bing; pang bo早期的经典算法的启发实现无监督的方法；2、cnn、rnn、svm等（建议参考semeval历届评测的code）

    3、核心创新点： 未来的创新点也许就在： 无监督或者半监督方法（迁移或者开放域的考虑）与预训练模型如bert等的结合上。清华大学刘知远（深度学习时代用hownet搞事情，记得是acl）

    4 可视化（验证如何使用）。自己开发一个web界面，调用自己的平台实现一种应用。一是基本功能的可视化（参见semeval的标准，和它兼容）；二是具体各种应用的可视化（例如舆情应用）。

    无监督或者半监督，腾讯游戏语料。

    目标：顶会的demo、学术影响力、

```python
    <sentence id="813">

    ​     <text>All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!</text>

    ​     <aspectTerms>

    ​          <aspectTerm term="appetizers" polarity="positive" from="8" to="18"/>

    ​          <aspectTerm term="salads" polarity="positive" from="23" to="29"/>

    ​          <aspectTerm term="steak" polarity="positive" from="49" to="54"/>

    ​          <aspectTerm term="pasta" polarity="positive" from="82" to="87"/>

    ​     </aspectTerms>

    ​     <aspectCategories>

    ​          <aspectCategory category="food" polarity="positive"/>

    ​     </aspectCategories>

    </sentence>
```

```
信息流中评论预测及挖掘技术研究
输入：一个帖子（属性：题目、正文之类的）（可选：背景，可以理解为若干文档的集合）
输出：预测评论如何：


重点是语料爬取。后边的工作如果有时间也可以先实现一个baseline
参考论文 NPP: A neural popularity prediction model for social media content。
1. 语料的获取。
   暂定 10个伊朗问题。来源最好是今日头条，或者。比如说篇新闻，每篇100条评论以上。语料最后有一定规模。
   http://comment5.news.sina.com.cn/comment/skin/default.html?channel=gj&newsid=comos-ihnzahk3365284&group=0
   也可以调研是否有类似的公开语料。

爬虫：
新浪新闻
微博
头条
网易新闻

话题：伊朗、中日、南海、等20个话题，每个话题至少100篇新闻文档、平均每个新闻200个评论以上。
来源必要时可以混杂。
数据结构，用excel pandas

2. 问题1： 给定新闻文章D，评论的主题分布预测。主题分布可以简单的理解为1）关键词的集合、
   训练语料：M篇新闻，及其对应的评论。
   方法1:1）对每个新闻的评论进行主题分布提取计算；主题模型（LDA）
             2）计算 D与M篇新闻的相似度，而后提取最相似的N篇文档，并利用其主题综合得到主题分布DIS。
            3）性能评价。
 sklearn

3. 问题2：  给定新闻文章D，预测评论中情感分布。例如，预测伊朗：4.8 等等。最好是限定若干情感aspect以及类别。
 情感分析：可以采用细颗粒度或者粗颗粒度（因为句子比较短）的方法，进行句子一级的提取。如果是粗颗粒度，则可以采用迁移的方法。
具体方法：可以和问题1类似。
```
